# Use the latest 2.1 version of CircleCI pipeline process engine.
# See: https://circleci.com/docs/2.0/configuration-reference
version: 2.1

# Define a job to be invoked later in a workflow.
# See: https://circleci.com/docs/2.0/configuration-reference/#jobs
orbs:
  aws-cli: circleci/aws-cli@3.1
  #aws-ecs: circleci/aws-ecs@2.0

commands:
  # Exercise: Reusable Job Code
  print_pipeline_id:
    parameters:
      id: 
        type: string
    steps:
      - run: echo << parameters.id >>

  # Exercise - Rollback
  destroy_environment:
    steps:
      - run:
          name: Destroy environment
          # ${CIRCLE_WORKFLOW_ID} is a Built-in environment variable 
          # ${CIRCLE_WORKFLOW_ID:0:5} takes the first 5 chars of the variable CIRCLE_CI_WORKFLOW_ID 
          when: on_fail
          command: aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5} --region us-east-1

jobs:
  create-inventory-file:
    # Specify the execution environment. You can specify an image from Dockerhub or use one of our Convenience Images from CircleCI's Developer Hub.
    # See: https://circleci.com/docs/2.0/configuration-reference/#docker-machine-macos-windows-executor
    docker:
      - image: cimg/python:3.9.1
    # Add steps to the job
    # See: https://circleci.com/docs/2.0/configuration-reference/#steps
    steps:
      - checkout
      - run:
          name: "Make directory"
          command: mkdir -p instances
      - run:
          name: "create inventory file"
          command: touch inventory > instances/inventory.txt
      - persist_to_workspace:
          root: instances
          paths:
            - inventory.txt
      - run:
          name: "create first line in file"
          command: echo [all] >> instances/inventory.txt
      - aws-cli/setup:
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-region: US_EAST_1_REGION
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
      - run:
          name: "query ec2 instances and append ip addresses to inventory file"
          command: aws ec2 describe-instances --query 'Reservations[*].Instances[*].PublicIpAddress' --filters "Name = tag:Name,Values = play_server" --output text >> instances/inventory.txt

  create-infrastructure: 
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
              --template-file infra-template.yml \
              --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5} \
              --region us-east-1
      # Fail the job intentionally to simulate an error.
      # Uncomment the line below if you want to fail the current step
      # - run: return 1
      #- destroy_environment
  # Config and Deployment
  configure-infrastructure: 
    docker:
      - image: python:3.9-alpine
    steps:
      - checkout
      - add_ssh_keys:
              # You can get this ID in the section where you registered the SSH Key
              fingerprints: "53:d6:66:21:ae:92:ca:dc:ff:a0:ea:1f:40:8c:07:94" 
      - run:
          name: Install dependencies
          command: |
            # install the dependencies needed for your playbook
            apk add --update ansible
      - run:
          name: Configure server
          command: ansible-playbook -i inventory main-remote.yml
  
  # Smoke Testing
  smoke-test:
    docker:
      - image: alpine:latest
    steps:
      - aws-cli/setup:
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-region: US_EAST_1_REGION
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
      - run: apk add --update curl
      - run:
          name: smoke test
          command: |
            URL="https://blog.udacity45.com/"
            # Test if website exists
            if curl -s --head ${URL} 
            then
              return 0
            else
              return 1
            fi
      - destroy_environment
  # Job 1
  # Executes the bucket.yml - Deploy an S3 bucket, and interface with that bucket to synchronize the files between local and the bucket.
  # Note that the `--parameter-overrides` let you specify a value that override parameter value in the bucket.yml template file.
  create_and_deploy_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - aws-cli/setup:
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-region: US_EAST_1_REGION
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
      - run:
          name: Execute bucket.yml - Create Cloudformation Stack
          command: |
            aws cloudformation deploy \
            --template-file bucket.yml \
            --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7} \
            --parameter-overrides MyBucketName="mybucket-${CIRCLE_WORKFLOW_ID:0:7}"
      # Uncomment the step below if you wish to upload all contents of the current directory to the S3 bucket
      - run: 
          name: Copy files to new S3 bucket
          command: |
            aws s3 sync \
            . s3://mybucket-${CIRCLE_WORKFLOW_ID:0:7} \
            --delete \
            --exclude 'bin/*' \
            --exclude 'include/*' \
            --exclude 'lib/*' \
            --exclude 'lib64/*' \
            --exclude 'share/*'
  # Exercise: Promote to Production - Job 2
  # Fetch and save the pipeline ID (bucket ID) responsible for the last release.
  get_last_deployment_id:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - aws-cli/setup:
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-region: US_EAST_1_REGION
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
      - run: yum install -y tar gzip
      - run:
          name: Fetch and save the old pipeline ID (bucket name) responsible for the last release.
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`PipelineID\`].Value" \
            --no-paginate --output text > ~/textfile.txt
      - persist_to_workspace:
          root: ~/
          paths: 
            - textfile.txt
  # Exercise: Promote to Production - Job 3
  # Executes the cloudfront.yml template that will modify the existing CloudFront Distribution, change its target from the old bucket to the new bucket - `mybucket-${CIRCLE_WORKFLOW_ID:0:7}`. 
  # Notice here we use the stack name `production-distro` which is the same name we used while deploying to the S3 bucket manually.
  promote_to_production:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - aws-cli/setup:
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-region: US_EAST_1_REGION
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
      - run:
          name: Execute cloudfront.yml
          command: |
            aws cloudformation deploy \
            --template-file cloudfront.yml \
            --stack-name production-distro \
            --parameter-overrides PipelineID="mybucket-${CIRCLE_WORKFLOW_ID:0:7}" 
  # Exercise: Promote to Production - Job 4
  # Destroy the previous production version's S3 bucket and CloudFormation stack. 
  clean_up_old_front_end:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - aws-cli/setup:
          aws-access-key-id: AWS_ACCESS_KEY_ID
          aws-region: US_EAST_1_REGION
          aws-secret-access-key: AWS_SECRET_ACCESS_KEY
      - run: yum install -y tar gzip
      - attach_workspace:
          at: ~/
      - run:
          name: Destroy the previous production version's S3 bucket and CloudFormation stack. 
          # Use $OldBucketID environment variable or mybucket644752792305 below.
          # Similarly, you can create and use $OldStackID environment variable in place of production-distro 
          command: |
            export OldBucketID=$(cat ~/textfile.txt)
            aws s3 rm "s3://${OldBucketID}" --recursive
          #  aws cloudformation delete-stack --stack-name production-distro 
          #  aws cloudformation delete-stack --stack-name stack-create-bucket-${CIRCLE_WORKFLOW_ID:0:7}
          #  aws cloudformation delete-stack --stack-name myStack-${CIRCLE_WORKFLOW_ID:0:5}

# Invoke jobs via workflows
# See: https://circleci.com/docs/2.0/configuration-reference/#workflows
workflows:
  doks-cool-workflow:
    jobs:
        - create_and_deploy_front_end
        - promote_to_production:
            requires: 
              - create_and_deploy_front_end
        - get_last_deployment_id
        - clean_up_old_front_end:
            requires:
              - get_last_deployment_id
              - promote_to_production
      #- create-inventory-file
      #- create-infrastructure
      #- configure-infrastructure
      #- smoke-test:
      #    requires:
      #      - create-infrastructure
